{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13527dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import cohen_kappa_score, confusion_matrix\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path('.').resolve().parent / 'src'))\n",
    "\n",
    "from config import RAW_DATA_DIR, PROCESSED_DATA_DIR, TEST_FOLDER\n",
    "from utils import load_json_annotations, parse_label_studio_export, clean_text\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed0a242",
   "metadata": {},
   "source": [
    "## 1. Load Raw Consensus Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e303f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find consensus directory\n",
    "consensus_dir = None\n",
    "for path in RAW_DATA_DIR.rglob(TEST_FOLDER):\n",
    "    if path.is_dir():\n",
    "        consensus_dir = path\n",
    "        break\n",
    "\n",
    "if consensus_dir:\n",
    "    print(f'Found consensus directory: {consensus_dir}')\n",
    "    json_files = list(consensus_dir.glob('*.json'))\n",
    "    print(f'Found {len(json_files)} JSON files')\n",
    "else:\n",
    "    print('Consensus directory not found')\n",
    "    json_files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c42dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all annotations from different annotators\n",
    "annotator_data = {}\n",
    "\n",
    "for json_file in json_files:\n",
    "    annotator_name = json_file.stem\n",
    "    try:\n",
    "        data = load_json_annotations(json_file)\n",
    "        parsed = parse_label_studio_export(data)\n",
    "        annotator_data[annotator_name] = parsed\n",
    "        print(f'{annotator_name}: {len(parsed)} annotations')\n",
    "    except Exception as e:\n",
    "        print(f'Error loading {annotator_name}: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dce075e",
   "metadata": {},
   "source": [
    "## 2. Build Annotation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6439f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping of texts to annotations from each annotator\n",
    "text_annotations = defaultdict(dict)\n",
    "\n",
    "for annotator, annotations in annotator_data.items():\n",
    "    for text, rating in annotations:\n",
    "        clean = clean_text(text)\n",
    "        text_annotations[clean][annotator] = rating\n",
    "\n",
    "print(f'Total unique texts: {len(text_annotations)}')\n",
    "\n",
    "# Create DataFrame\n",
    "rows = []\n",
    "for text, annotations in text_annotations.items():\n",
    "    row = {'text': text}\n",
    "    for annotator in annotator_data.keys():\n",
    "        row[annotator] = annotations.get(annotator, np.nan)\n",
    "    rows.append(row)\n",
    "\n",
    "annotation_df = pd.DataFrame(rows)\n",
    "print(f'\\nDataFrame shape: {annotation_df.shape}')\n",
    "display(annotation_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58443892",
   "metadata": {},
   "source": [
    "## 3. Inter-Annotator Agreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b44d407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get annotator columns\n",
    "annotator_cols = [col for col in annotation_df.columns if col != 'text']\n",
    "\n",
    "if len(annotator_cols) >= 2:\n",
    "    # Pairwise Cohen's Kappa\n",
    "    kappa_matrix = np.zeros((len(annotator_cols), len(annotator_cols)))\n",
    "    \n",
    "    for i, ann1 in enumerate(annotator_cols):\n",
    "        for j, ann2 in enumerate(annotator_cols):\n",
    "            # Get common annotations\n",
    "            mask = annotation_df[[ann1, ann2]].notna().all(axis=1)\n",
    "            if mask.sum() > 0:\n",
    "                ratings1 = annotation_df.loc[mask, ann1].values\n",
    "                ratings2 = annotation_df.loc[mask, ann2].values\n",
    "                try:\n",
    "                    kappa = cohen_kappa_score(ratings1, ratings2)\n",
    "                    kappa_matrix[i, j] = kappa\n",
    "                except:\n",
    "                    kappa_matrix[i, j] = np.nan\n",
    "            else:\n",
    "                kappa_matrix[i, j] = np.nan\n",
    "    \n",
    "    # Plot heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(kappa_matrix, annot=True, fmt='.2f', \n",
    "                xticklabels=annotator_cols, yticklabels=annotator_cols,\n",
    "                cmap='RdYlGn', vmin=-1, vmax=1, center=0)\n",
    "    plt.title(\"Pairwise Cohen's Kappa Between Annotators\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Average Kappa\n",
    "    valid_kappas = kappa_matrix[~np.isnan(kappa_matrix) & (np.eye(len(annotator_cols)) == 0)]\n",
    "    print(f'\\nAverage pairwise Kappa: {np.mean(valid_kappas):.3f}')\n",
    "else:\n",
    "    print('Not enough annotators for inter-annotator agreement analysis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef46122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate agreement for each text\n",
    "def calculate_agreement(row):\n",
    "    ratings = row[annotator_cols].dropna().values\n",
    "    if len(ratings) < 2:\n",
    "        return np.nan, np.nan, np.nan\n",
    "    \n",
    "    # Majority vote\n",
    "    counter = Counter(ratings)\n",
    "    majority = counter.most_common(1)[0]\n",
    "    agreement = majority[1] / len(ratings)\n",
    "    \n",
    "    # Standard deviation\n",
    "    std = np.std(ratings)\n",
    "    \n",
    "    return majority[0], agreement, std\n",
    "\n",
    "if len(annotator_cols) >= 2:\n",
    "    results = annotation_df.apply(calculate_agreement, axis=1)\n",
    "    annotation_df['consensus_label'] = [r[0] for r in results]\n",
    "    annotation_df['agreement'] = [r[1] for r in results]\n",
    "    annotation_df['std'] = [r[2] for r in results]\n",
    "    \n",
    "    print('Agreement Statistics:')\n",
    "    print(annotation_df[['agreement', 'std']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b506dd8b",
   "metadata": {},
   "source": [
    "## 4. Label Distribution by Annotator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fc8272",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(annotator_cols) > 0:\n",
    "    # Create distribution comparison\n",
    "    fig, axes = plt.subplots(1, min(len(annotator_cols), 4), figsize=(16, 4))\n",
    "    if len(annotator_cols) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, annotator in enumerate(annotator_cols[:4]):\n",
    "        ratings = annotation_df[annotator].dropna()\n",
    "        counts = ratings.value_counts().sort_index()\n",
    "        \n",
    "        axes[i].bar(counts.index, counts.values, color='steelblue', alpha=0.7)\n",
    "        axes[i].set_xlabel('Rating')\n",
    "        axes[i].set_ylabel('Count')\n",
    "        axes[i].set_title(f'{annotator}')\n",
    "        axes[i].set_xticks([1, 2, 3, 4, 5])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47275bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare annotator means\n",
    "if len(annotator_cols) > 0:\n",
    "    annotator_stats = []\n",
    "    for annotator in annotator_cols:\n",
    "        ratings = annotation_df[annotator].dropna()\n",
    "        annotator_stats.append({\n",
    "            'Annotator': annotator,\n",
    "            'Count': len(ratings),\n",
    "            'Mean': ratings.mean(),\n",
    "            'Std': ratings.std(),\n",
    "            'Min': ratings.min(),\n",
    "            'Max': ratings.max()\n",
    "        })\n",
    "    \n",
    "    stats_df = pd.DataFrame(annotator_stats)\n",
    "    display(stats_df.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757bcd9e",
   "metadata": {},
   "source": [
    "## 5. Disagreement Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b33af59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find texts with highest disagreement\n",
    "if 'agreement' in annotation_df.columns:\n",
    "    disagreements = annotation_df.nsmallest(10, 'agreement')[['text', 'consensus_label', 'agreement', 'std'] + annotator_cols]\n",
    "    \n",
    "    print('Top 10 Most Disputed Texts:')\n",
    "    print('=' * 60)\n",
    "    \n",
    "    for idx, row in disagreements.iterrows():\n",
    "        print(f'\\nText: {row[\"text\"][:100]}...')\n",
    "        print(f'Consensus: {row[\"consensus_label\"]}, Agreement: {row[\"agreement\"]:.2%}')\n",
    "        ratings = [f'{ann}: {row[ann]}' for ann in annotator_cols if pd.notna(row[ann])]\n",
    "        print(f'Ratings: {ratings}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c28d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disagreement by rating category\n",
    "if 'consensus_label' in annotation_df.columns and 'agreement' in annotation_df.columns:\n",
    "    agreement_by_label = annotation_df.groupby('consensus_label')['agreement'].agg(['mean', 'std', 'count'])\n",
    "    \n",
    "    print('\\nAgreement by Consensus Label:')\n",
    "    display(agreement_by_label.round(3))\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.bar(agreement_by_label.index, agreement_by_label['mean'], \n",
    "            yerr=agreement_by_label['std'], capsize=5, color='teal', alpha=0.7)\n",
    "    plt.xlabel('Consensus Rating')\n",
    "    plt.ylabel('Average Agreement')\n",
    "    plt.title('Annotator Agreement by Rating Category')\n",
    "    plt.xticks([1, 2, 3, 4, 5])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a414b9c",
   "metadata": {},
   "source": [
    "## 6. Confusion Between Adjacent Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4cf90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze confusion patterns between annotators\n",
    "if len(annotator_cols) >= 2:\n",
    "    all_confusions = np.zeros((5, 5))\n",
    "    \n",
    "    for i, ann1 in enumerate(annotator_cols):\n",
    "        for j, ann2 in enumerate(annotator_cols):\n",
    "            if i < j:  # Only count each pair once\n",
    "                mask = annotation_df[[ann1, ann2]].notna().all(axis=1)\n",
    "                if mask.sum() > 0:\n",
    "                    ratings1 = annotation_df.loc[mask, ann1].astype(int).values\n",
    "                    ratings2 = annotation_df.loc[mask, ann2].astype(int).values\n",
    "                    cm = confusion_matrix(ratings1, ratings2, labels=[1, 2, 3, 4, 5])\n",
    "                    all_confusions += cm\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(all_confusions, annot=True, fmt='.0f', cmap='Blues',\n",
    "                xticklabels=[1, 2, 3, 4, 5], yticklabels=[1, 2, 3, 4, 5])\n",
    "    plt.xlabel('Annotator 2 Rating')\n",
    "    plt.ylabel('Annotator 1 Rating')\n",
    "    plt.title('Overall Confusion Matrix Between Annotators')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294d5db4",
   "metadata": {},
   "source": [
    "## 7. Save Processed Consensus Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0489c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the annotation analysis\n",
    "if 'consensus_label' in annotation_df.columns:\n",
    "    # Clean and save\n",
    "    final_df = annotation_df[['text', 'consensus_label', 'agreement', 'std']].dropna()\n",
    "    final_df = final_df.rename(columns={'consensus_label': 'label'})\n",
    "    final_df['label'] = final_df['label'].astype(int)\n",
    "    \n",
    "    output_path = PROCESSED_DATA_DIR / 'consensus_analysis.csv'\n",
    "    final_df.to_csv(output_path, index=False)\n",
    "    print(f'Saved consensus analysis to {output_path}')\n",
    "    print(f'Total samples: {len(final_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7069695a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print('\\n' + '=' * 60)\n",
    "print('LABEL ANALYSIS SUMMARY')\n",
    "print('=' * 60)\n",
    "\n",
    "if len(annotator_cols) > 0:\n",
    "    print(f'\\nNumber of annotators: {len(annotator_cols)}')\n",
    "    print(f'Total unique texts: {len(annotation_df)}')\n",
    "    \n",
    "    if 'agreement' in annotation_df.columns:\n",
    "        print(f'\\nAgreement Statistics:')\n",
    "        print(f'  Mean agreement: {annotation_df[\"agreement\"].mean():.2%}')\n",
    "        print(f'  High agreement (>75%): {(annotation_df[\"agreement\"] > 0.75).sum()} texts')\n",
    "        print(f'  Low agreement (<50%): {(annotation_df[\"agreement\"] < 0.5).sum()} texts')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
