{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3bbc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path('.').resolve().parent / 'src'))\n",
    "\n",
    "from config import RAW_DATA_DIR, PROCESSED_DATA_DIR, TRAIN_FOLDER, TEST_FOLDER\n",
    "from utils import clean_text, extract_text_features, load_json_annotations, parse_label_studio_export\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2687ef6a",
   "metadata": {},
   "source": [
    "## 1. Adatok betöltése"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72673f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed data\n",
    "train_path = PROCESSED_DATA_DIR / 'train.csv'\n",
    "test_path = PROCESSED_DATA_DIR / 'test.csv'\n",
    "\n",
    "if train_path.exists():\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    print(f'Training samples: {len(train_df)}')\n",
    "    display(train_df.head())\n",
    "else:\n",
    "    print('Training data not found. Please run 01_data_preprocessing.py first.')\n",
    "\n",
    "if test_path.exists():\n",
    "    test_df = pd.read_csv(test_path)\n",
    "    print(f'\\nTest samples: {len(test_df)}')\n",
    "    display(test_df.head())\n",
    "else:\n",
    "    print('Test data not found.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02cb39c",
   "metadata": {},
   "source": [
    "## 2. Label Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765829ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create label distribution plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Training data\n",
    "if 'train_df' in dir():\n",
    "    train_counts = train_df['label'].value_counts().sort_index()\n",
    "    axes[0].bar(train_counts.index, train_counts.values, color='steelblue', alpha=0.8)\n",
    "    axes[0].set_xlabel('Rating')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[0].set_title('Training Data - Label Distribution')\n",
    "    axes[0].set_xticks([1, 2, 3, 4, 5])\n",
    "    \n",
    "    for i, v in enumerate(train_counts.values):\n",
    "        axes[0].text(train_counts.index[i], v + 1, str(v), ha='center')\n",
    "\n",
    "# Test data\n",
    "if 'test_df' in dir():\n",
    "    test_counts = test_df['label'].value_counts().sort_index()\n",
    "    axes[1].bar(test_counts.index, test_counts.values, color='coral', alpha=0.8)\n",
    "    axes[1].set_xlabel('Rating')\n",
    "    axes[1].set_ylabel('Count')\n",
    "    axes[1].set_title('Test Data (Consensus) - Label Distribution')\n",
    "    axes[1].set_xticks([1, 2, 3, 4, 5])\n",
    "    \n",
    "    for i, v in enumerate(test_counts.values):\n",
    "        axes[1].text(test_counts.index[i], v + 1, str(v), ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2b727d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed statistics\n",
    "print('Training Data Statistics:')\n",
    "print('=' * 40)\n",
    "if 'train_df' in dir():\n",
    "    for label in sorted(train_df['label'].unique()):\n",
    "        count = (train_df['label'] == label).sum()\n",
    "        pct = count / len(train_df) * 100\n",
    "        print(f'Rating {label}: {count:4d} ({pct:5.1f}%)')\n",
    "\n",
    "print('\\nTest Data Statistics:')\n",
    "print('=' * 40)\n",
    "if 'test_df' in dir():\n",
    "    for label in sorted(test_df['label'].unique()):\n",
    "        count = (test_df['label'] == label).sum()\n",
    "        pct = count / len(test_df) * 100\n",
    "        print(f'Rating {label}: {count:4d} ({pct:5.1f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd89c29d",
   "metadata": {},
   "source": [
    "## 3. Text Length Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95e8440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add text features\n",
    "def add_text_features(df):\n",
    "    df = df.copy()\n",
    "    df['word_count'] = df['text'].apply(lambda x: len(str(x).split()))\n",
    "    df['char_count'] = df['text'].apply(lambda x: len(str(x)))\n",
    "    df['avg_word_length'] = df['text'].apply(\n",
    "        lambda x: np.mean([len(w) for w in str(x).split()]) if str(x).split() else 0\n",
    "    )\n",
    "    return df\n",
    "\n",
    "if 'train_df' in dir():\n",
    "    train_df = add_text_features(train_df)\n",
    "if 'test_df' in dir():\n",
    "    test_df = add_text_features(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff7bb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text length distribution by rating\n",
    "if 'train_df' in dir():\n",
    "    fig = px.box(train_df, x='label', y='word_count', \n",
    "                 title='Word Count Distribution by Rating (Training Data)',\n",
    "                 labels={'label': 'Rating', 'word_count': 'Word Count'})\n",
    "    fig.update_layout(xaxis_title='Rating (1=Hard, 5=Easy)')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c063c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation between text length and rating\n",
    "if 'train_df' in dir():\n",
    "    correlation = train_df['word_count'].corr(train_df['label'])\n",
    "    print(f'Correlation between word count and rating: {correlation:.3f}')\n",
    "    \n",
    "    # Average word count by rating\n",
    "    avg_by_rating = train_df.groupby('label')['word_count'].mean()\n",
    "    print('\\nAverage word count by rating:')\n",
    "    for rating, avg in avg_by_rating.items():\n",
    "        print(f'  Rating {rating}: {avg:.1f} words')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b53c56",
   "metadata": {},
   "source": [
    "## 4. Sample Texts by Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6b88fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample texts for each rating\n",
    "rating_descriptions = {\n",
    "    1: 'Nagyon nehezen vagy nem értelmezhető',\n",
    "    2: 'Nehezen értelmezhető',\n",
    "    3: 'Valamennyire érthető',\n",
    "    4: 'Végigolvasva megértem',\n",
    "    5: 'Könnyen, egyből érthető'\n",
    "}\n",
    "\n",
    "if 'train_df' in dir():\n",
    "    for rating in [1, 2, 3, 4, 5]:\n",
    "        samples = train_df[train_df['label'] == rating]['text'].head(2)\n",
    "        print(f'\\n{\"=\"*60}')\n",
    "        print(f'Rating {rating}: {rating_descriptions[rating]}')\n",
    "        print('=' * 60)\n",
    "        for i, text in enumerate(samples, 1):\n",
    "            print(f'\\nSample {i}:')\n",
    "            print(f'{text[:300]}...' if len(text) > 300 else text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec0e03e",
   "metadata": {},
   "source": [
    "## 5. Annotator Agreement Analysis (Test Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154243ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'test_df' in dir() and 'agreement' in test_df.columns:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Agreement distribution\n",
    "    axes[0].hist(test_df['agreement'], bins=20, color='teal', alpha=0.7, edgecolor='black')\n",
    "    axes[0].set_xlabel('Agreement Score')\n",
    "    axes[0].set_ylabel('Count')\n",
    "    axes[0].set_title('Annotator Agreement Distribution')\n",
    "    axes[0].axvline(test_df['agreement'].mean(), color='red', linestyle='--', \n",
    "                    label=f'Mean: {test_df[\"agreement\"].mean():.2f}')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Agreement by rating\n",
    "    agreement_by_rating = test_df.groupby('label')['agreement'].mean()\n",
    "    axes[1].bar(agreement_by_rating.index, agreement_by_rating.values, \n",
    "                color='teal', alpha=0.7)\n",
    "    axes[1].set_xlabel('Rating')\n",
    "    axes[1].set_ylabel('Average Agreement')\n",
    "    axes[1].set_title('Average Agreement by Rating')\n",
    "    axes[1].set_xticks([1, 2, 3, 4, 5])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print('\\nAgreement Statistics:')\n",
    "    print(f'Mean: {test_df[\"agreement\"].mean():.3f}')\n",
    "    print(f'Std: {test_df[\"agreement\"].std():.3f}')\n",
    "    print(f'Min: {test_df[\"agreement\"].min():.3f}')\n",
    "    print(f'Max: {test_df[\"agreement\"].max():.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519eeab1",
   "metadata": {},
   "source": [
    "## 6. Word Frequency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97de9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def get_word_freq(texts, top_n=20):\n",
    "    all_words = []\n",
    "    for text in texts:\n",
    "        words = re.findall(r'\\b\\w+\\b', str(text).lower())\n",
    "        all_words.extend(words)\n",
    "    return Counter(all_words).most_common(top_n)\n",
    "\n",
    "# Most common words by rating\n",
    "if 'train_df' in dir():\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Overall\n",
    "    word_freq = get_word_freq(train_df['text'], 15)\n",
    "    words, counts = zip(*word_freq)\n",
    "    axes[0].barh(words, counts, color='gray')\n",
    "    axes[0].set_title('Overall Most Common Words')\n",
    "    axes[0].invert_yaxis()\n",
    "    \n",
    "    # By rating\n",
    "    for i, rating in enumerate([1, 2, 3, 4, 5], 1):\n",
    "        texts = train_df[train_df['label'] == rating]['text']\n",
    "        word_freq = get_word_freq(texts, 15)\n",
    "        if word_freq:\n",
    "            words, counts = zip(*word_freq)\n",
    "            axes[i].barh(words, counts, color=plt.cm.RdYlGn(rating/5))\n",
    "            axes[i].set_title(f'Rating {rating}')\n",
    "            axes[i].invert_yaxis()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7388a15a",
   "metadata": {},
   "source": [
    "## 7. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e0aa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary table\n",
    "summary_data = []\n",
    "\n",
    "if 'train_df' in dir():\n",
    "    summary_data.append({\n",
    "        'Dataset': 'Training',\n",
    "        'Samples': len(train_df),\n",
    "        'Avg Word Count': train_df['word_count'].mean(),\n",
    "        'Avg Char Count': train_df['char_count'].mean(),\n",
    "        'Label Mean': train_df['label'].mean(),\n",
    "        'Label Std': train_df['label'].std()\n",
    "    })\n",
    "\n",
    "if 'test_df' in dir():\n",
    "    summary_data.append({\n",
    "        'Dataset': 'Test (Consensus)',\n",
    "        'Samples': len(test_df),\n",
    "        'Avg Word Count': test_df['word_count'].mean(),\n",
    "        'Avg Char Count': test_df['char_count'].mean(),\n",
    "        'Label Mean': test_df['label'].mean(),\n",
    "        'Label Std': test_df['label'].std()\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "display(summary_df.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09860f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save summary\n",
    "if len(summary_data) > 0:\n",
    "    summary_df.to_csv(PROCESSED_DATA_DIR / 'data_summary.csv', index=False)\n",
    "    print(f'Summary saved to {PROCESSED_DATA_DIR / \"data_summary.csv\"}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
