# Legal Text Decoder

## Deep Learning (VITMMA19) Project Work

---

## Project Information

| Field | Value |
|-------|-------|
| **Selected Topic** | Legal Text Decoder |
| **Student Name** | Petrich Tam√°s |
| **Neptun Code** | FA0B9B |
| **Aiming for +1 Mark** | Yes |

---

## Solution Description

### Problem Statement

A projekt c√©lja egy term√©szetes nyelvfeldolgoz√°si (NLP) modell l√©trehoz√°sa, amely k√©pes megj√≥solni, hogy egy adott √Åltal√°nos Szerz≈ëd√©si Felt√©telek (√ÅSZF) sz√∂veg√©nek egy bekezd√©se mennyire k√∂nnyen vagy nehezen √©rthet≈ë egy √°tlagos felhaszn√°l√≥ sz√°m√°ra. A modell egy 1-t≈ël 5-ig terjed≈ë sk√°l√°n adja meg az √©rthet≈ës√©get.

### Rating Scale

| Rating | Description |
|--------|-------------|
| 1 | Nagyon nehezen vagy nem √©rtelmezhet≈ë |
| 2 | Nehezen √©rtelmezhet≈ë |
| 3 | Valamennyire √©rthet≈ë, de er≈ësen kell koncentr√°lni |
| 4 | V√©gigolvasva meg√©rtem |
| 5 | K√∂nnyen, egyb≈ël √©rthet≈ë |

### Model Architecture

A projekt k√©t f≈ë modellt tartalmaz:

#### 1. Baseline Model (TF-IDF + Logistic Regression)
- **Feature Extraction**: TF-IDF vectorizer (max 5000 features, unigrams + bigrams)
- **Classifier**: Multinomial Logistic Regression with class balancing
- **Purpose**: Gyors, interpreth√°lhat√≥ baseline eredm√©nyek

#### 2. Transformer Model (HuBERT)
- **Pre-trained Model**: SZTAKI-HLT/hubert-base-cc (Hungarian BERT)
- **Architecture**: BERT encoder + classification head
- **Max Sequence Length**: 256 tokens
- **Training**: Fine-tuned with AdamW optimizer, linear warmup scheduler
- **Regularization**: Dropout (0.1), Early stopping, Class weighting

### Training Methodology

1. **Data Loading**: Label Studio JSON exports feldolgoz√°sa
2. **Preprocessing**: Sz√∂veg tiszt√≠t√°s, tokeniz√°ci√≥
3. **Training/Validation Split**: 80/20 stratified split
4. **Training**: Cross-entropy loss with class weights
5. **Model Selection**: Early stopping based on validation F1 score

### Evaluation Metrics

- **Accuracy**: Overall classification accuracy
- **F1 Score (Macro/Weighted)**: Class-balanced performance
- **Mean Absolute Error (MAE)**: Rating prediction error
- **Cohen's Kappa**: Inter-rater agreement proxy
- **Confusion Matrix**: Detailed error analysis

---

## Extra Credit Justification

A k√∂vetkez≈ë elemek miatt p√°ly√°zom a +1 jegyre:

1. **√Åtfog√≥ megold√°s**: Baseline √©s fejlett transformer modell √∂sszehasonl√≠t√°sa
2. **Magyar nyelvi modell**: HuBERT fine-tuning specifikusan magyar jogi sz√∂vegekre
3. **Consensus alap√∫ tesztel√©s**: T√∂bb annot√°tor egyet√©rt√©s√©nek figyelembev√©tele
4. **R√©szletes ki√©rt√©kel√©s**: T√∂bbf√©le metrika, confusion matrix, model comparison
5. **Tiszta, modul√°ris k√≥d**: J√≥l struktur√°lt, dokument√°lt Python k√≥d
6. **Docker kontaineriz√°ci√≥**: Teljes reproduk√°lhat√≥s√°g

---

## Data Preparation

### Data Source
- **Training**: FA0B9B neptun k√≥dos mappa annot√°ci√≥i
- **Test**: Consensus mappa (t√∂bb annot√°tor k√∂z√∂s c√≠mk√©z√©se)

### Processing Steps

1. **Download**: A SharePoint linkr≈ël automatikusan let√∂ltj√ºk az adatokat
2. **Extract**: ZIP f√°jl kicsomagol√°sa
3. **Parse**: Label Studio JSON export form√°tum feldolgoz√°sa
4. **Clean**: Sz√∂veg tiszt√≠t√°s (whitespace, special characters)
5. **Consensus Calculation**: Teszt adatokn√°l majority voting

### Data Format

**Input (Label Studio JSON)**:
```json
{
  "data": {"text": "Jogi sz√∂veg..."},
  "annotations": [{"result": [{"value": {"choices": ["3-T√∂bb√©/kev√©sb√© meg√©rtem"]}}]}]
}
```

**Output (CSV)**:
```csv
text,label
"Jogi sz√∂veg...",3
```

---

## üöÄ Gyors Futtat√°s

### M√≥dszer 1: Quick Start Script (Windows)

```powershell
.\quick_start.ps1
```

Ez automatikusan:
- ‚úÖ Ellen≈ërzi a Python verzi√≥t
- ‚úÖ L√©trehozza a virtu√°lis k√∂rnyezetet
- ‚úÖ Telep√≠ti a f√ºgg≈ës√©geket
- ‚úÖ Futtatja a teljes pipeline-t

### M√≥dszer 2: Manu√°lis (Lok√°lisan)

```powershell
# 1. Virtu√°lis k√∂rnyezet
python -m venv venv
.\venv\Scripts\Activate.ps1

# 2. F√ºgg≈ës√©gek
pip install -r requirements.txt

# 3. Python path
$env:PYTHONPATH = "$PWD\src"

# 4. Futtat√°s
python main.py
```

**V√°rhat√≥ fut√°sid≈ë**: 30-60 perc (CPU), 10-20 perc (GPU)

### M√≥dszer 3: Docker (B√°rhol)

```bash
# Build
docker build -t legal-text-decoder .

# Run - teljes pipeline
docker run --rm \
  -v $(pwd)/data:/app/data \
  -v $(pwd)/models:/app/models \
  -v $(pwd)/log:/app/log \
  legal-text-decoder

# Windows PowerShell:
docker run --rm `
  -v ${PWD}/data:/app/data `
  -v ${PWD}/models:/app/models `
  -v ${PWD}/log:/app/log `
  legal-text-decoder

# GPU t√°mogat√°ssal
docker run --rm --gpus all \
  -v $(pwd)/data:/app/data \
  -v $(pwd)/models:/app/models \
  legal-text-decoder
```

### L√©p√©senk√©nti Futtat√°s

```powershell
# Csak adat feldolgoz√°s
python src\a01_data_preprocessing.py

# Csak tan√≠t√°s
python src\a02_training.py

# Csak ki√©rt√©kel√©s
python src\a03_evaluation.py

# Inference egyetlen sz√∂vegre
python src\a04_inference.py --text "Az √ÅSZF m√≥dos√≠t√°s√°r√≥l e-mailben √©rtes√≠tj√ºk."

# Interakt√≠v m√≥d
python src\a04_inference.py --interactive
```

**üìñ R√©szletes √∫tmutat√≥**: L√°sd [HOGYAN_FUTTASSAM.md](HOGYAN_FUTTASSAM.md)

---

## File Structure

```
LegalTextDecoder/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ config.py              # Configuration and hyperparameters
‚îÇ   ‚îú‚îÄ‚îÄ utils.py               # Utility functions and logging
‚îÇ   ‚îú‚îÄ‚îÄ 01_data_preprocessing.py   # Data loading and preparation
‚îÇ   ‚îú‚îÄ‚îÄ 02_training.py         # Model training (baseline + transformer)
‚îÇ   ‚îú‚îÄ‚îÄ 03_evaluation.py       # Model evaluation on test set
‚îÇ   ‚îî‚îÄ‚îÄ 04_inference.py        # Prediction on new texts
‚îú‚îÄ‚îÄ notebook/
‚îÇ   ‚îú‚îÄ‚îÄ 01_data_exploration.ipynb  # EDA and visualization
‚îÇ   ‚îî‚îÄ‚îÄ 02_label_analysis.ipynb    # Label distribution analysis
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/                   # Downloaded data
‚îÇ   ‚îî‚îÄ‚îÄ processed/             # Prepared train/test CSVs
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ baseline_model.pkl     # Trained baseline model
‚îÇ   ‚îî‚îÄ‚îÄ transformer/           # Trained transformer model
‚îú‚îÄ‚îÄ log/
‚îÇ   ‚îî‚îÄ‚îÄ run.log               # Training and evaluation logs
‚îú‚îÄ‚îÄ Dockerfile                # Docker configuration
‚îú‚îÄ‚îÄ requirements.txt          # Python dependencies
‚îî‚îÄ‚îÄ README.md                 # This file
```

---

## Configuration

Key hyperparameters (in `src/config.py`):

### Baseline Model
| Parameter | Value |
|-----------|-------|
| TF-IDF Max Features | 5000 |
| N-gram Range | (1, 2) |
| Classifier | Logistic Regression |
| Class Weight | Balanced |

### Transformer Model
| Parameter | Value |
|-----------|-------|
| Model | SZTAKI-HLT/hubert-base-cc |
| Max Length | 256 |
| Batch Size | 16 |
| Learning Rate | 2e-5 |
| Epochs | 10 |
| Warmup Ratio | 0.1 |
| Dropout | 0.1 |
| Early Stopping | 3 epochs |

---

## Results

*Results will be populated after training*

### Validation Set

| Model | Accuracy | F1 (Macro) | F1 (Weighted) | MAE |
|-------|----------|------------|---------------|-----|
| Baseline | - | - | - | - |
| Transformer | - | - | - | - |

### Test Set (Consensus)

| Model | Accuracy | F1 (Macro) | F1 (Weighted) | MAE |
|-------|----------|------------|---------------|-----|
| Baseline | - | - | - | - |
| Transformer | - | - | - | - |

---

## Usage Examples

### Python API

```python
from src.04_inference import load_models, predict_single

# Load models
baseline, transformer, tokenizer = load_models(MODEL_DIR, 'cuda', logger)

# Predict
text = "A Szolg√°ltat√≥ fenntartja a jogot..."
result = predict_single(text, baseline, transformer, tokenizer, 'cuda')

print(f"Rating: {result['ensemble']['prediction']}")
print(f"Description: {result['ensemble']['description']}")
```

### Command Line

```bash
# Single text
python src/04_inference.py --text "Jogi sz√∂veg..."

# File prediction
python src/04_inference.py --input texts.txt --output predictions.csv

# Interactive mode
python src/04_inference.py --interactive
```

---

## Requirements

- Python 3.10+
- PyTorch 2.0+
- CUDA 11.8+ (optional, for GPU support)
- See `requirements.txt` for full list

---

## License

This project was created for educational purposes as part of the Deep Learning (VITMMA19) course at BME.
